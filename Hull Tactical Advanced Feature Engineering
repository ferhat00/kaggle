{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":111543,"databundleVersionId":14348714,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸš€ Hull Tactical - ADVANCED FEATURE ENGINEERING (500+ Features)\n\n**This notebook implements comprehensive quantitative features:**\n\n- ðŸ“Š **Technical Indicators**: 50+ trend, momentum, volatility indicators\n- ðŸ“ˆ **Statistical Features**: Rolling stats, correlations, regressions\n- ðŸ’¹ **Volume Features**: Liquidity, flow indicators\n- ðŸŽ¯ **Risk Metrics**: VaR, beta, downside deviation\n- ðŸ”¬ **Advanced Math**: Entropy, Hurst exponent, wavelets, PCA\n- ðŸ¤ **Interactions**: Cross-features and nonlinear transforms\n\n**Total Features: 500+** (vs 200 in basic version)\n\n---","metadata":{}},{"cell_type":"markdown","source":"## ðŸ“¦ Enhanced Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom scipy import stats\nfrom scipy.signal import welch\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import RobustScaler, QuantileTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import TimeSeriesSplit\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# GPU Detection\ntry:\n    import torch\n    GPU_AVAILABLE = torch.cuda.is_available()\n    if GPU_AVAILABLE:\n        print(f\"ðŸŽ® GPU: {torch.cuda.get_device_name(0)}\")\nexcept:\n    GPU_AVAILABLE = False\n    print(\"ðŸ’» CPU Mode\")\n\n# Model libraries\ntry:\n    import xgboost as xgb\n    XGBOOST_GPU = GPU_AVAILABLE\n    print(f\"âœ… XGBoost: {'GPU' if XGBOOST_GPU else 'CPU'}\")\nexcept:\n    from sklearn.ensemble import RandomForestRegressor\n    XGBOOST_GPU = False\n    print(\"âœ… RandomForest: CPU\")\n\nimport kaggle_evaluation.default_inference_server\n\nprint(\"âœ… All imports loaded!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:56:38.714502Z","iopub.execute_input":"2025-11-07T20:56:38.714692Z","iopub.status.idle":"2025-11-07T20:56:46.841845Z","shell.execute_reply.started":"2025-11-07T20:56:38.714674Z","shell.execute_reply":"2025-11-07T20:56:46.841214Z"}},"outputs":[{"name":"stdout","text":"ðŸŽ® GPU: Tesla T4\nâœ… XGBoost: GPU\nâœ… All imports loaded!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## âš™ï¸ Configuration with Feature Toggles","metadata":{}},{"cell_type":"code","source":"CONFIG = {\n    # Model Settings\n    'n_estimators': 800 if GPU_AVAILABLE else 500,\n    'max_depth': 12,\n    'learning_rate': 0.05,\n    'use_gpu': GPU_AVAILABLE,\n    \n    # Position Sizing\n    'kelly_fraction': 0.26,\n    'confidence_threshold': 0.0006,\n    'min_position': 0.0,\n    'max_position': 2.0,\n    'default_position': 1.0,\n    'defensive_position': 0.5,\n    \n    # Feature Engineering Toggles (Turn on/off feature groups)\n    'features': {\n        'basic_technical': True,      # MA, EMA, RSI, MACD, BB\n        'advanced_technical': True,   # ADX, Aroon, Keltner, Ichimoku\n        'momentum_extended': True,    # CMO, PPO, Stochastic\n        'volume_features': True,      # OBV, CMF, MFI, A/D\n        'lagged_features': True,      # t-1, t-2, ... t-n lags\n        'rolling_stats': True,        # Mean, std, quantiles\n        'rolling_correlations': True, # Price-vol, asset-benchmark\n        'rolling_regressions': True,  # Trend slopes\n        'entropy_features': True,     # Shannon entropy, Hurst\n        'autocorrelation': True,      # Persistence measures\n        'risk_metrics': True,         # VaR, beta, downside dev\n        'returns_multiperiod': True,  # 1D, 3D, 5D, 10D, 20D returns\n        'statistical_tests': True,    # Z-scores, residuals\n        'pca_features': True,         # Dimensionality reduction\n        'clustering_regime': True,    # Market regime detection\n        'nonlinear_transforms': True, # Log, sqrt, rank, quantile\n        'interaction_features': True, # priceÃ—volume, returnÃ—vol\n        'fourier_features': False,    # Cyclical components (slow)\n        'wavelet_features': False,    # Wavelet transforms (slow)\n    },\n    \n    # Feature Parameters\n    'n_base_features': 30,           # Top N base features to use\n    'lag_periods': [1, 2, 3, 5, 10, 20],\n    'ma_windows': [5, 10, 20, 50, 100, 200],\n    'momentum_periods': [3, 5, 10, 14, 20, 30],\n    'vol_windows': [5, 10, 20, 60, 120],\n    'corr_windows': [20, 60, 120],\n    'regression_windows': [20, 60],\n    'quantiles': [0.1, 0.25, 0.5, 0.75, 0.9],\n    'pca_components': 20,\n    'n_clusters': 5,\n    \n    'random_state': 42,\n}\n\nprint(\"âš™ï¸ Configuration loaded\")\nprint(f\"  GPU: {CONFIG['use_gpu']}\")\nprint(f\"  Estimators: {CONFIG['n_estimators']}\")\nprint(f\"  Feature Groups Enabled: {sum(CONFIG['features'].values())}/{len(CONFIG['features'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:56:46.843055Z","iopub.execute_input":"2025-11-07T20:56:46.843389Z","iopub.status.idle":"2025-11-07T20:56:46.851133Z","shell.execute_reply.started":"2025-11-07T20:56:46.843371Z","shell.execute_reply":"2025-11-07T20:56:46.850279Z"}},"outputs":[{"name":"stdout","text":"âš™ï¸ Configuration loaded\n  GPU: True\n  Estimators: 800\n  Feature Groups Enabled: 17/19\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## ðŸ”¬ Comprehensive Feature Engineering Library","metadata":{}},{"cell_type":"code","source":"class AdvancedFeatureEngineer:\n    \"\"\"\n    Comprehensive feature engineering for financial time series\n    Implements 500+ features across multiple categories\n    \"\"\"\n    \n    def __init__(self, config=CONFIG):\n        self.config = config\n        self.feature_count = 0\n    \n    # ============================================================\n    # 1. BASIC TECHNICAL INDICATORS\n    # ============================================================\n    \n    def add_basic_technical(self, df, cols):\n        \"\"\"Basic technical indicators: MA, EMA, RSI, MACD, Bollinger Bands\"\"\"\n        print(\"  â†’ Basic Technical Indicators...\")\n        \n        for col in cols:\n            # Moving Averages\n            for w in self.config['ma_windows']:\n                df[f'{col}_sma_{w}'] = df[col].rolling(w, min_periods=1).mean()\n                df[f'{col}_ema_{w}'] = df[col].ewm(span=w, min_periods=1).mean()\n                df[f'{col}_wma_{w}'] = df[col].rolling(w, min_periods=1).apply(\n                    lambda x: np.average(x, weights=np.arange(1, len(x)+1)) if len(x) > 0 else np.nan\n                )\n            \n            # RSI\n            delta = df[col].diff()\n            gain = delta.where(delta > 0, 0).rolling(14, min_periods=1).mean()\n            loss = -delta.where(delta < 0, 0).rolling(14, min_periods=1).mean()\n            rs = gain / (loss + 1e-10)\n            df[f'{col}_rsi'] = 100 - (100 / (1 + rs))\n            \n            # MACD\n            ema_12 = df[col].ewm(span=12).mean()\n            ema_26 = df[col].ewm(span=26).mean()\n            df[f'{col}_macd'] = ema_12 - ema_26\n            df[f'{col}_macd_signal'] = df[f'{col}_macd'].ewm(span=9).mean()\n            df[f'{col}_macd_diff'] = df[f'{col}_macd'] - df[f'{col}_macd_signal']\n            \n            # Bollinger Bands\n            for w in [20, 50]:\n                sma = df[col].rolling(w, min_periods=1).mean()\n                std = df[col].rolling(w, min_periods=1).std()\n                df[f'{col}_bb_upper_{w}'] = sma + 2*std\n                df[f'{col}_bb_lower_{w}'] = sma - 2*std\n                df[f'{col}_bb_width_{w}'] = (df[f'{col}_bb_upper_{w}'] - df[f'{col}_bb_lower_{w}']) / (sma + 1e-10)\n                df[f'{col}_bb_pct_{w}'] = (df[col] - df[f'{col}_bb_lower_{w}']) / (df[f'{col}_bb_upper_{w}'] - df[f'{col}_bb_lower_{w}'] + 1e-10)\n        \n        return df\n    \n    # ============================================================\n    # 2. ADVANCED TECHNICAL INDICATORS\n    # ============================================================\n    \n    def add_advanced_technical(self, df, cols):\n        \"\"\"ADX, Aroon, Keltner Channels, Parabolic SAR, Vortex\"\"\"\n        print(\"  â†’ Advanced Technical Indicators...\")\n        \n        for col in cols:\n            # ADX (Average Directional Index)\n            high = df[col].rolling(2).max()\n            low = df[col].rolling(2).min()\n            tr = high - low\n            df[f'{col}_adx'] = tr.rolling(14, min_periods=1).mean()\n            \n            # Aroon Indicator\n            for w in [25, 50]:\n                aroon_up = df[col].rolling(w).apply(lambda x: (w - x.argmax()) / w * 100)\n                aroon_down = df[col].rolling(w).apply(lambda x: (w - x.argmin()) / w * 100)\n                df[f'{col}_aroon_up_{w}'] = aroon_up\n                df[f'{col}_aroon_down_{w}'] = aroon_down\n                df[f'{col}_aroon_osc_{w}'] = aroon_up - aroon_down\n            \n            # Keltner Channels\n            ema_20 = df[col].ewm(span=20).mean()\n            atr = df[col].diff().abs().rolling(20, min_periods=1).mean()\n            df[f'{col}_keltner_upper'] = ema_20 + 2*atr\n            df[f'{col}_keltner_lower'] = ema_20 - 2*atr\n            df[f'{col}_keltner_pct'] = (df[col] - df[f'{col}_keltner_lower']) / (df[f'{col}_keltner_upper'] - df[f'{col}_keltner_lower'] + 1e-10)\n            \n            # Vortex Indicator\n            vm_plus = (df[col] - df[col].shift(1)).abs()\n            vm_minus = (df[col].shift(1) - df[col]).abs()\n            tr_14 = df[col].diff().abs().rolling(14, min_periods=1).sum()\n            df[f'{col}_vortex_pos'] = vm_plus.rolling(14, min_periods=1).sum() / (tr_14 + 1e-10)\n            df[f'{col}_vortex_neg'] = vm_minus.rolling(14, min_periods=1).sum() / (tr_14 + 1e-10)\n        \n        return df\n    \n    # ============================================================\n    # 3. MOMENTUM INDICATORS (EXTENDED)\n    # ============================================================\n    \n    def add_momentum_extended(self, df, cols):\n        \"\"\"CMO, PPO, Stochastic, ROC, Williams %R\"\"\"\n        print(\"  â†’ Extended Momentum Indicators...\")\n        \n        for col in cols:\n            # Rate of Change (multiple periods)\n            for p in self.config['momentum_periods']:\n                df[f'{col}_roc_{p}'] = df[col].pct_change(p)\n                df[f'{col}_momentum_{p}'] = df[col].diff(p)\n            \n            # Chande Momentum Oscillator (CMO)\n            delta = df[col].diff()\n            su = delta.where(delta > 0, 0).rolling(14, min_periods=1).sum()\n            sd = -delta.where(delta < 0, 0).rolling(14, min_periods=1).sum()\n            df[f'{col}_cmo'] = 100 * (su - sd) / (su + sd + 1e-10)\n            \n            # Percentage Price Oscillator (PPO)\n            ema_12 = df[col].ewm(span=12).mean()\n            ema_26 = df[col].ewm(span=26).mean()\n            df[f'{col}_ppo'] = 100 * (ema_12 - ema_26) / (ema_26 + 1e-10)\n            \n            # Stochastic Oscillator\n            for w in [14, 20]:\n                low_min = df[col].rolling(w, min_periods=1).min()\n                high_max = df[col].rolling(w, min_periods=1).max()\n                df[f'{col}_stoch_k_{w}'] = 100 * (df[col] - low_min) / (high_max - low_min + 1e-10)\n                df[f'{col}_stoch_d_{w}'] = df[f'{col}_stoch_k_{w}'].rolling(3).mean()\n            \n            # Williams %R\n            df[f'{col}_willr'] = -100 * (df[col].rolling(14).max() - df[col]) / (df[col].rolling(14).max() - df[col].rolling(14).min() + 1e-10)\n        \n        return df\n    \n    # ============================================================\n    # 4. VOLUME FEATURES\n    # ============================================================\n    \n    def add_volume_features(self, df, price_cols, has_returns=True):\n        \"\"\"OBV, CMF, MFI, A/D, Volume indicators\"\"\"\n        print(\"  â†’ Volume Features...\")\n        \n        # Create proxy volume from volatility (since we don't have actual volume)\n        if has_returns:\n            proxy_volume = df['forward_returns'].abs().rolling(5, min_periods=1).mean()\n        else:\n            proxy_volume = pd.Series(1, index=df.index)\n        \n        for col in price_cols:\n            # On-Balance Volume (OBV)\n            obv = (np.sign(df[col].diff()) * proxy_volume).fillna(0).cumsum()\n            df[f'{col}_obv'] = obv\n            df[f'{col}_obv_ema'] = obv.ewm(span=20).mean()\n            \n            # Volume-based features\n            df[f'{col}_volume_sma_20'] = proxy_volume.rolling(20, min_periods=1).mean()\n            df[f'{col}_volume_std_20'] = proxy_volume.rolling(20, min_periods=1).std()\n            df[f'{col}_volume_ratio'] = proxy_volume / (df[f'{col}_volume_sma_20'] + 1e-10)\n            \n            # Ease of Movement\n            distance = (df[col] + df[col].shift(1)) / 2 - (df[col].shift(1) + df[col].shift(2)) / 2\n            df[f'{col}_eom'] = distance / (proxy_volume + 1e-10)\n            df[f'{col}_eom_ma'] = df[f'{col}_eom'].rolling(14, min_periods=1).mean()\n        \n        return df\n    \n    # ============================================================\n    # 5. LAGGED FEATURES\n    # ============================================================\n    \n    def add_lagged_features(self, df, cols):\n        \"\"\"Lagged values: t-1, t-2, ... t-n\"\"\"\n        print(\"  â†’ Lagged Features...\")\n        \n        for col in cols:\n            for lag in self.config['lag_periods']:\n                df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n        \n        return df\n    \n    # ============================================================\n    # 6. ROLLING STATISTICS\n    # ============================================================\n    \n    def add_rolling_stats(self, df, cols):\n        \"\"\"Mean, std, min, max, median, quantiles, skew, kurt\"\"\"\n        print(\"  â†’ Rolling Statistics...\")\n        \n        for col in cols:\n            for w in [10, 20, 60]:\n                df[f'{col}_mean_{w}'] = df[col].rolling(w, min_periods=3).mean()\n                df[f'{col}_std_{w}'] = df[col].rolling(w, min_periods=3).std()\n                df[f'{col}_min_{w}'] = df[col].rolling(w, min_periods=3).min()\n                df[f'{col}_max_{w}'] = df[col].rolling(w, min_periods=3).max()\n                df[f'{col}_median_{w}'] = df[col].rolling(w, min_periods=3).median()\n                df[f'{col}_range_{w}'] = df[f'{col}_max_{w}'] - df[f'{col}_min_{w}']\n                df[f'{col}_iqr_{w}'] = df[col].rolling(w, min_periods=5).quantile(0.75) - df[col].rolling(w, min_periods=5).quantile(0.25)\n                \n                if w >= 20:\n                    df[f'{col}_skew_{w}'] = df[col].rolling(w, min_periods=10).skew()\n                    df[f'{col}_kurt_{w}'] = df[col].rolling(w, min_periods=10).kurt()\n                \n                # Quantiles\n                for q in self.config['quantiles']:\n                    df[f'{col}_q{int(q*100)}_{w}'] = df[col].rolling(w, min_periods=5).quantile(q)\n                \n                # Z-score\n                df[f'{col}_zscore_{w}'] = (df[col] - df[f'{col}_mean_{w}']) / (df[f'{col}_std_{w}'] + 1e-10)\n        \n        return df\n    \n    # ============================================================\n    # 7. ROLLING CORRELATIONS\n    # ============================================================\n    \n    def add_rolling_correlations(self, df, cols):\n        \"\"\"Rolling correlations between features\"\"\"\n        print(\"  â†’ Rolling Correlations...\")\n        \n        # Correlations between top features\n        if len(cols) >= 2:\n            for w in self.config['corr_windows']:\n                # Pairwise correlations for first few features\n                for i in range(min(3, len(cols))):\n                    for j in range(i+1, min(5, len(cols))):\n                        corr = df[cols[i]].rolling(w, min_periods=10).corr(df[cols[j]])\n                        df[f'corr_{cols[i]}_{cols[j]}_{w}'] = corr\n        \n        return df\n    \n    # ============================================================\n    # 8. ROLLING REGRESSIONS\n    # ============================================================\n    \n    def add_rolling_regressions(self, df, cols):\n        \"\"\"Rolling regression slopes and intercepts\"\"\"\n        print(\"  â†’ Rolling Regressions...\")\n        \n        for col in cols:\n            for w in self.config['regression_windows']:\n                # Linear trend\n                def calc_slope(x):\n                    if len(x) < 3:\n                        return np.nan\n                    X = np.arange(len(x)).reshape(-1, 1)\n                    y = x.values\n                    slope = np.polyfit(X.flatten(), y, 1)[0]\n                    return slope\n                \n                df[f'{col}_slope_{w}'] = df[col].rolling(w, min_periods=5).apply(calc_slope, raw=False)\n                \n                # R-squared (trend strength)\n                def calc_r2(x):\n                    if len(x) < 3:\n                        return np.nan\n                    X = np.arange(len(x))\n                    y = x.values\n                    slope, intercept = np.polyfit(X, y, 1)\n                    y_pred = slope * X + intercept\n                    ss_res = np.sum((y - y_pred) ** 2)\n                    ss_tot = np.sum((y - np.mean(y)) ** 2)\n                    return 1 - ss_res / (ss_tot + 1e-10)\n                \n                df[f'{col}_trend_strength_{w}'] = df[col].rolling(w, min_periods=5).apply(calc_r2, raw=False)\n        \n        return df\n    \n    # ============================================================\n    # 9. ENTROPY AND COMPLEXITY\n    # ============================================================\n    \n    def add_entropy_features(self, df, cols):\n        \"\"\"Shannon entropy, approximate entropy, Hurst exponent\"\"\"\n        print(\"  â†’ Entropy Features...\")\n        \n        for col in cols:\n            # Shannon Entropy\n            def shannon_entropy(x):\n                if len(x) < 5:\n                    return np.nan\n                hist, _ = np.histogram(x, bins=10, density=True)\n                hist = hist[hist > 0]\n                return -np.sum(hist * np.log2(hist + 1e-10))\n            \n            df[f'{col}_entropy_20'] = df[col].rolling(20, min_periods=10).apply(shannon_entropy, raw=True)\n            df[f'{col}_entropy_60'] = df[col].rolling(60, min_periods=20).apply(shannon_entropy, raw=True)\n            \n            # Hurst Exponent (simplified)\n            def hurst_simple(x):\n                if len(x) < 20:\n                    return np.nan\n                lags = range(2, min(20, len(x)//2))\n                tau = [np.std(np.subtract(x[lag:], x[:-lag])) for lag in lags]\n                return np.polyfit(np.log(lags), np.log(tau), 1)[0]\n            \n            df[f'{col}_hurst_60'] = df[col].rolling(60, min_periods=30).apply(hurst_simple, raw=True)\n        \n        return df\n    \n    # ============================================================\n    # 10. AUTOCORRELATION\n    # ============================================================\n    \n    def add_autocorrelation(self, df, cols):\n        \"\"\"Lag-1, lag-5, lag-10 autocorrelation\"\"\"\n        print(\"  â†’ Autocorrelation Features...\")\n        \n        for col in cols:\n            for lag in [1, 5, 10]:\n                df[f'{col}_autocorr_{lag}'] = df[col].rolling(30, min_periods=lag+5).apply(\n                    lambda x: x.autocorr(lag=lag) if len(x) > lag else np.nan, raw=False\n                )\n        \n        return df\n    \n    # ============================================================\n    # 11. RISK METRICS\n    # ============================================================\n    \n    def add_risk_metrics(self, df, return_col='forward_returns'):\n        \"\"\"VaR, beta, downside deviation, Sharpe, Sortino\"\"\"\n        print(\"  â†’ Risk Metrics...\")\n        \n        has_returns = return_col in df.columns\n        returns = df[return_col] if has_returns else pd.Series(np.nan, index=df.index)\n        \n        for w in [20, 60, 120]:\n            # Volatility\n            df[f'volatility_{w}'] = returns.rolling(w, min_periods=5).std()\n            \n            # Value at Risk (95%, 99%)\n            df[f'var_95_{w}'] = returns.rolling(w, min_periods=10).quantile(0.05)\n            df[f'var_99_{w}'] = returns.rolling(w, min_periods=10).quantile(0.01)\n            \n            # Downside Deviation\n            downside = returns[returns < 0].copy()\n            df[f'downside_dev_{w}'] = downside.rolling(w, min_periods=5).std()\n            \n            # Rolling Sharpe (simplified)\n            mean_ret = returns.rolling(w, min_periods=5).mean()\n            std_ret = returns.rolling(w, min_periods=5).std()\n            df[f'sharpe_{w}'] = mean_ret / (std_ret + 1e-10)\n            \n            # Rolling Sortino (simplified)\n            df[f'sortino_{w}'] = mean_ret / (df[f'downside_dev_{w}'] + 1e-10)\n            \n            # Max Drawdown\n            cumulative = (1 + returns).rolling(w, min_periods=5).apply(lambda x: x.prod(), raw=True)\n            running_max = cumulative.rolling(w, min_periods=5).max()\n            df[f'drawdown_{w}'] = (cumulative - running_max) / (running_max + 1e-10)\n        \n        return df\n    \n    # ============================================================\n    # 12. MULTI-PERIOD RETURNS\n    # ============================================================\n    \n    def add_multiperiod_returns(self, df, cols):\n        \"\"\"1D, 3D, 5D, 10D, 20D, 60D returns\"\"\"\n        print(\"  â†’ Multi-Period Returns...\")\n        \n        for col in cols:\n            for period in [1, 3, 5, 10, 20, 60]:\n                # Arithmetic returns\n                df[f'{col}_return_{period}d'] = df[col].pct_change(period)\n                \n                # Log returns\n                df[f'{col}_log_return_{period}d'] = np.log(df[col] / (df[col].shift(period) + 1e-10))\n                \n                # Cumulative returns\n                df[f'{col}_cum_return_{period}d'] = (1 + df[col].pct_change()).rolling(period).apply(\n                    lambda x: x.prod() - 1, raw=True\n                )\n        \n        return df\n    \n    # ============================================================\n    # 13. STATISTICAL TESTS & RESIDUALS\n    # ============================================================\n    \n    def add_statistical_tests(self, df, cols):\n        \"\"\"Z-scores, residuals from moving averages, distance metrics\"\"\"\n        print(\"  â†’ Statistical Tests...\")\n        \n        for col in cols:\n            # Distance from moving average\n            for w in [20, 50, 200]:\n                sma = df[col].rolling(w, min_periods=1).mean()\n                df[f'{col}_dist_sma_{w}'] = (df[col] - sma) / (sma + 1e-10)\n                df[f'{col}_above_sma_{w}'] = (df[col] > sma).astype(int)\n            \n            # Price relative to MAs\n            df[f'{col}_rel_sma_20'] = df[col] / (df[col].rolling(20, min_periods=1).mean() + 1e-10)\n            df[f'{col}_rel_sma_50'] = df[col] / (df[col].rolling(50, min_periods=1).mean() + 1e-10)\n            \n            # MA crossover signals\n            sma_20 = df[col].rolling(20, min_periods=1).mean()\n            sma_50 = df[col].rolling(50, min_periods=1).mean()\n            df[f'{col}_ma_cross_20_50'] = (sma_20 > sma_50).astype(int)\n        \n        return df\n    \n    # ============================================================\n    # 14. PCA FEATURES\n    # ============================================================\n    \n    def add_pca_features(self, df, cols, n_components=20):\n        \"\"\"PCA dimensionality reduction on base features\"\"\"\n        print(f\"  â†’ PCA Features ({n_components} components)...\")\n        \n        # Get data matrix\n        X = df[cols].fillna(0).values\n        \n        if X.shape[0] > n_components * 2:\n            pca = PCA(n_components=min(n_components, X.shape[1]))\n            X_pca = pca.fit_transform(X)\n            \n            for i in range(X_pca.shape[1]):\n                df[f'pca_{i+1}'] = X_pca[:, i]\n        \n        return df\n    \n    # ============================================================\n    # 15. CLUSTERING / REGIME DETECTION\n    # ============================================================\n    \n    def add_clustering_regime(self, df, cols, n_clusters=5):\n        \"\"\"K-means clustering for market regime detection\"\"\"\n        print(f\"  â†’ Market Regime Clustering ({n_clusters} regimes)...\")\n        \n        # Use first few features for clustering\n        feature_subset = cols[:min(10, len(cols))]\n        X = df[feature_subset].fillna(method='ffill').fillna(0).values\n        \n        if X.shape[0] > n_clusters * 10:\n            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n            df['market_regime'] = kmeans.fit_predict(X)\n            \n            # One-hot encode regimes\n            for i in range(n_clusters):\n                df[f'regime_{i}'] = (df['market_regime'] == i).astype(int)\n        \n        return df\n    \n    # ============================================================\n    # 16. NONLINEAR TRANSFORMS\n    # ============================================================\n    \n    def add_nonlinear_transforms(self, df, cols):\n        \"\"\"Log, sqrt, square, rank, quantile binning\"\"\"\n        print(\"  â†’ Nonlinear Transforms...\")\n        \n        for col in cols[:10]:  # Limit to avoid explosion\n            # Log transform (shift to positive if needed)\n            shifted = df[col] - df[col].min() + 1\n            df[f'{col}_log'] = np.log(shifted + 1e-10)\n            \n            # Square root\n            df[f'{col}_sqrt'] = np.sqrt(np.abs(df[col]))\n            \n            # Square\n            df[f'{col}_square'] = df[col] ** 2\n            \n            # Rank (percentile)\n            df[f'{col}_rank_pct'] = df[col].rank(pct=True)\n            \n            # Quantile binning\n            df[f'{col}_bin'] = pd.qcut(df[col], q=10, labels=False, duplicates='drop')\n        \n        return df\n    \n    # ============================================================\n    # 17. INTERACTION FEATURES\n    # ============================================================\n    \n    def add_interaction_features(self, df, cols, has_returns=True):\n        \"\"\"priceÃ—volume, returnÃ—volatility, feature products\"\"\"\n        print(\"  â†’ Interaction Features...\")\n        \n        # Cross-sectional aggregations\n        vol_cols = [c for c in cols if c.startswith('V')]\n        mom_cols = [c for c in cols if c.startswith('MOM')]\n        \n        if vol_cols:\n            df['vol_mean'] = df[vol_cols].mean(axis=1)\n            df['vol_std'] = df[vol_cols].std(axis=1)\n            df['vol_max'] = df[vol_cols].max(axis=1)\n            df['vol_min'] = df[vol_cols].min(axis=1)\n        \n        if mom_cols:\n            df['mom_mean'] = df[mom_cols].mean(axis=1)\n            df['mom_std'] = df[mom_cols].std(axis=1)\n        \n        # Interactions\n        if vol_cols and mom_cols:\n            df['vol_mom_product'] = df['vol_mean'] * df['mom_mean']\n            df['vol_mom_ratio'] = df['vol_mean'] / (abs(df['mom_mean']) + 1e-10)\n        \n        # Return Ã— Volatility\n        if has_returns and 'volatility_20' in df.columns:\n            df['return_vol_product'] = df['forward_returns'] * df['volatility_20']\n        \n        return df\n    \n    # ============================================================\n    # MAIN PIPELINE\n    # ============================================================\n    \n    def engineer_all_features(self, df, is_training=True):\n        \"\"\"\n        Main feature engineering pipeline\n        Applies all enabled feature groups\n        \"\"\"\n        print(\"\\n\" + \"=\"*70)\n        print(\"ðŸ”¬ ADVANCED FEATURE ENGINEERING\")\n        print(\"=\"*70)\n        \n        df = df.copy()\n        config = self.config\n        \n        # Get base columns\n        base_cols = [col for col in df.columns if col not in \n                    ['date_id', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns']]\n        \n        # Limit to top N\n        top_cols = base_cols[:config['n_base_features']]\n        print(f\"\\nðŸ“Š Using {len(top_cols)} base features from {len(base_cols)} available\")\n        \n        has_returns = 'forward_returns' in df.columns\n        \n        # Apply feature groups (only if enabled)\n        if config['features']['basic_technical']:\n            df = self.add_basic_technical(df, top_cols[:10])\n        \n        if config['features']['advanced_technical']:\n            df = self.add_advanced_technical(df, top_cols[:8])\n        \n        if config['features']['momentum_extended']:\n            df = self.add_momentum_extended(df, top_cols[:10])\n        \n        if config['features']['volume_features']:\n            df = self.add_volume_features(df, top_cols[:5], has_returns)\n        \n        if config['features']['lagged_features']:\n            df = self.add_lagged_features(df, top_cols[:15])\n        \n        if config['features']['rolling_stats']:\n            df = self.add_rolling_stats(df, top_cols[:10])\n        \n        if config['features']['rolling_correlations']:\n            df = self.add_rolling_correlations(df, top_cols)\n        \n        if config['features']['rolling_regressions']:\n            df = self.add_rolling_regressions(df, top_cols[:8])\n        \n        if config['features']['entropy_features']:\n            df = self.add_entropy_features(df, top_cols[:5])\n        \n        if config['features']['autocorrelation']:\n            df = self.add_autocorrelation(df, top_cols[:8])\n        \n        if config['features']['risk_metrics']:\n            df = self.add_risk_metrics(df)\n        \n        if config['features']['returns_multiperiod']:\n            df = self.add_multiperiod_returns(df, top_cols[:8])\n        \n        if config['features']['statistical_tests']:\n            df = self.add_statistical_tests(df, top_cols[:10])\n        \n        if config['features']['pca_features'] and is_training:\n            df = self.add_pca_features(df, top_cols, config['pca_components'])\n        \n        if config['features']['clustering_regime'] and is_training:\n            df = self.add_clustering_regime(df, top_cols, config['n_clusters'])\n        \n        if config['features']['nonlinear_transforms']:\n            df = self.add_nonlinear_transforms(df, top_cols)\n        \n        if config['features']['interaction_features']:\n            df = self.add_interaction_features(df, base_cols, has_returns)\n        \n        # Count features\n        feature_cols = [col for col in df.columns if col not in \n                       ['date_id', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns']]\n        \n        print(f\"\\nâœ… Feature Engineering Complete!\")\n        print(f\"   Total Features: {len(feature_cols)}\")\n        print(f\"   Base Features: {len(base_cols)}\")\n        print(f\"   Engineered Features: {len(feature_cols) - len(base_cols)}\")\n        print(\"=\"*70)\n        \n        return df\n\n\nprint(\"âœ… AdvancedFeatureEngineer class loaded!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:56:46.852235Z","iopub.execute_input":"2025-11-07T20:56:46.852500Z","iopub.status.idle":"2025-11-07T20:56:46.910639Z","shell.execute_reply.started":"2025-11-07T20:56:46.852479Z","shell.execute_reply":"2025-11-07T20:56:46.909872Z"}},"outputs":[{"name":"stdout","text":"âœ… AdvancedFeatureEngineer class loaded!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## ðŸ¤– Model with Advanced Features","metadata":{}},{"cell_type":"code","source":"class AdvancedMarketPredictor:\n    \"\"\"\n    Predictor using 500+ advanced features\n    \"\"\"\n    \n    def __init__(self, config=CONFIG):\n        self.config = config\n        self.feature_engineer = AdvancedFeatureEngineer(config)\n        \n        # Initialize model based on GPU availability\n        if config['use_gpu'] and XGBOOST_GPU:\n            print(\"ðŸš€ Using XGBoost GPU\")\n            self.model = xgb.XGBRegressor(\n                n_estimators=config['n_estimators'],\n                max_depth=config['max_depth'],\n                learning_rate=config['learning_rate'],\n                tree_method='gpu_hist',\n                gpu_id=0,\n                predictor='gpu_predictor',\n                random_state=config['random_state']\n            )\n        else:\n            print(\"ðŸ’» Using RandomForest CPU\")\n            from sklearn.ensemble import RandomForestRegressor\n            self.model = RandomForestRegressor(\n                n_estimators=config['n_estimators'],\n                max_depth=config['max_depth'],\n                min_samples_split=20,\n                random_state=config['random_state'],\n                n_jobs=-1\n            )\n        \n        self.scaler = None\n        self.imputer = None\n        self.feature_cols = None\n        self.train_vol = 0.01\n    \n    def fit(self, train_df):\n        \"\"\"Train with advanced features\"\"\"\n        import time\n        start = time.time()\n        \n        print(\"\\n\" + \"=\"*70)\n        print(\"ðŸš€ TRAINING ADVANCED MODEL\")\n        print(\"=\"*70)\n        \n        # Feature engineering\n        train_df = self.feature_engineer.engineer_all_features(train_df, is_training=True)\n        \n        # Get features\n        self.feature_cols = [col for col in train_df.columns if col not in \n                            ['date_id', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns']]\n        \n        print(f\"\\nðŸ“Š Preparing {len(self.feature_cols)} features...\")\n        \n        X = train_df[self.feature_cols].values\n        y = train_df['forward_returns'].values\n        \n        # Handle inf/nan\n        X = np.nan_to_num(X, nan=np.nan, posinf=np.nan, neginf=np.nan)\n        \n        # Impute and scale\n        self.imputer = SimpleImputer(strategy='median')\n        X = self.imputer.fit_transform(X)\n        \n        self.scaler = RobustScaler()\n        X = self.scaler.fit_transform(X)\n        \n        # Remove NaN targets\n        valid = ~np.isnan(y)\n        X, y = X[valid], y[valid]\n        \n        print(f\"âœ… Training samples: {len(X):,}\")\n        print(f\"âœ… Feature dimensions: {X.shape[1]}\")\n        \n        # Train\n        print(\"\\nðŸ”¨ Training model...\")\n        self.model.fit(X, y)\n        \n        self.train_vol = train_df['forward_returns'].std()\n        \n        elapsed = time.time() - start\n        print(f\"\\nâ±ï¸ Training time: {elapsed:.1f} seconds\")\n        print(\"âœ… Training complete!\")\n        \n        return self\n    \n    def predict_position(self, test_df, risk_free_rate=0.02/252):\n        \"\"\"Predict optimal position\"\"\"\n        # Feature engineering\n        test_df = self.feature_engineer.engineer_all_features(test_df, is_training=False)\n        \n        # Prepare features\n        X = test_df[self.feature_cols].values\n        X = np.nan_to_num(X, nan=np.nan, posinf=np.nan, neginf=np.nan)\n        X = self.imputer.transform(X)\n        X = self.scaler.transform(X)\n        \n        # Predict\n        pred_return = self.model.predict(X)[0]\n        \n        # Kelly position sizing\n        excess = pred_return - risk_free_rate\n        \n        if excess > self.config['confidence_threshold']:\n            position = self.config['kelly_fraction'] * (excess / (self.train_vol ** 2))\n            position = np.clip(position, 0, self.config['max_position'])\n            if position > 1.3:\n                position = 1.0 + (position - 1.0) * 0.5\n        elif excess > -self.config['confidence_threshold']:\n            position = self.config['default_position']\n        else:\n            position = self.config['defensive_position']\n        \n        return float(np.clip(position, self.config['min_position'], self.config['max_position']))\n\n\nprint(\"âœ… AdvancedMarketPredictor loaded!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:56:46.911470Z","iopub.execute_input":"2025-11-07T20:56:46.911672Z","iopub.status.idle":"2025-11-07T20:56:46.927882Z","shell.execute_reply.started":"2025-11-07T20:56:46.911653Z","shell.execute_reply":"2025-11-07T20:56:46.927307Z"}},"outputs":[{"name":"stdout","text":"âœ… AdvancedMarketPredictor loaded!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## ðŸ“Š Train Model","metadata":{}},{"cell_type":"code","source":"print(\"ðŸ“¥ Loading data...\")\ntrain_df = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/train.csv')\n\nprint(f\"Dataset: {train_df.shape}\")\nprint(f\"Date range: {train_df['date_id'].min()} to {train_df['date_id'].max()}\")\n\n# Train\nmodel = AdvancedMarketPredictor(config=CONFIG)\nmodel.fit(train_df)\n\nprint(\"\\nðŸŽ‰ Ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:56:46.929252Z","iopub.execute_input":"2025-11-07T20:56:46.929501Z"}},"outputs":[{"name":"stdout","text":"ðŸ“¥ Loading data...\nDataset: (9021, 98)\nDate range: 0 to 9020\nðŸš€ Using XGBoost GPU\n\n======================================================================\nðŸš€ TRAINING ADVANCED MODEL\n======================================================================\n\n======================================================================\nðŸ”¬ ADVANCED FEATURE ENGINEERING\n======================================================================\n\nðŸ“Š Using 30 base features from 94 available\n  â†’ Basic Technical Indicators...\n  â†’ Advanced Technical Indicators...\n  â†’ Extended Momentum Indicators...\n  â†’ Volume Features...\n  â†’ Lagged Features...\n  â†’ Rolling Statistics...\n  â†’ Rolling Correlations...\n  â†’ Rolling Regressions...\n  â†’ Entropy Features...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## ðŸŽ¯ Inference","metadata":{}},{"cell_type":"code","source":"def predict(test: pl.DataFrame) -> float:\n    test_pd = test.to_pandas()\n    risk_free_rate = 0.02 / 252\n    if 'risk_free_rate' in test_pd.columns:\n        risk_free_rate = test_pd['risk_free_rate'].values[0]\n    position = model.predict_position(test_pd, risk_free_rate=risk_free_rate)\n    return position\n\ninference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}